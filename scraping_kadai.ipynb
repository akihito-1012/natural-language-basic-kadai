{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib import request\n",
        "\n",
        "url = 'https://www.aozora.gr.jp/cards/000148/files/2371_13943.html'\n",
        "response = request.urlopen(url)\n",
        "soup = BeautifulSoup(response)\n",
        "response.close()\n",
        "\n",
        "main_text = soup.find('div', class_='main_text')\n",
        "tags_to_delete = main_text.find_all(['rp', 'rt'])\n",
        "for tag in tags_to_delete:\n",
        "    tag.decompose()\n",
        "\n",
        "main_text = main_text.get_text()\n",
        "\n",
        "import re\n",
        "main_text = re.sub(r\"[¥u3000 ¥n ¥r]\", \"\", main_text)\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib import request\n",
        "\n",
        "url = 'http://svn.sourceforge.jp/svnroot/slothlib/CSharp/Version1/SlothLib/NLP/Filter/StopWord/word/Japanese.txt'\n",
        "response = request.urlopen(url)\n",
        "soup = BeautifulSoup(response)\n",
        "response.close()\n",
        "\n",
        "stopwords_text = soup.text\n",
        "stopwords_list = stopwords_text.split(\"¥r¥n\")\n",
        "stopwords_list = [word for word in stopwords_list if word]\n",
        "\n",
        "split_text_list = ['近頃', 'は', '大分', '方々', 'の', '雑誌', 'から', '談話', 'を', 'しろ', 'しろ', 'と', '責め', 'られて', '、', '頭', 'が', 'がらん', '胴',' に', 'なった', 'から', '、', '当分', '品切れ', 'の', '看板', 'でも', '懸け', 'たい', 'くらい', 'に', '思って', 'い', 'ます', '。' ]\n",
        "result_text_list = list()\n",
        "for split_text in split_text_list:\n",
        "  if split_text not in stopwords_list:\n",
        "    result_text_list.append(split_text)\n",
        "\n",
        "print(result_text_list)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HD5vHX5JT53E",
        "outputId": "2c4ec168-27a6-4f01-b2c9-c37d0dba778c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.12.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.6)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-c1c94ab7fd0e>:8: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
            "  soup = BeautifulSoup(response)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['近頃', 'は', '大分', '方々', 'の', '雑誌', 'から', '談話', 'を', 'しろ', 'しろ', 'と', '責め', 'られて', '、', '頭', 'が', 'がらん', '胴', ' に', 'なった', 'から', '、', '当分', '品切れ', 'の', '看板', 'でも', '懸け', 'たい', 'くらい', 'に', '思って', 'い', 'ます', '。']\n"
          ]
        }
      ]
    }
  ]
}